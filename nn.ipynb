{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2424ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zeyad\\miniconda3\\lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from enviornment import WumpusEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a7c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_tensor(state, H, W):\n",
    "    \"\"\"\n",
    "    Single H x W integer grid flattened to length H*W.\n",
    "    Codes (ordered low->high as you wanted):\n",
    "      0 : pit\n",
    "      1 : wumpus_up\n",
    "      2 : wumpus_down\n",
    "      3 : wumpus_left\n",
    "      4 : wumpus_right\n",
    "      5 : exit\n",
    "      6 : gold\n",
    "      7 : agent (no gold)\n",
    "      8 : agent (has gold)\n",
    "    Returns: 1D torch.float32 tensor of length H*W\n",
    "    \"\"\"\n",
    "    grid = np.full((H, W), 2.0, dtype=np.float32)  # default = empty (use 2 to sit between wumpus and exit)\n",
    "\n",
    "    # pits\n",
    "    for pr, pc in state[\"pits\"]:\n",
    "        grid[pr, pc] = 0.0\n",
    "\n",
    "    # wumpuses with facing preserved\n",
    "    face_code = {\"up\": 1.0, \"down\": 2.0, \"left\": 3.0, \"right\": 4.0}\n",
    "    for pos in state[\"wumpus\"]:\n",
    "        fr, fc = pos\n",
    "        fac = state[\"wumpus_facing\"].get(pos, \"down\")\n",
    "        grid[fr, fc] = face_code.get(fac, 2.0)\n",
    "\n",
    "    # exit\n",
    "    er, ec = state[\"exit\"]\n",
    "    grid[er, ec] = 5.0\n",
    "\n",
    "    # gold (if present)\n",
    "    if state[\"gold\"] is not None:\n",
    "        gr, gc = state[\"gold\"]\n",
    "        grid[gr, gc] = 6.0\n",
    "\n",
    "    ar, ac = state[\"agent\"]\n",
    "    if state.get(\"have_gold\", False):\n",
    "        grid[ar, ac] = 8.0\n",
    "    else:\n",
    "        grid[ar, ac] = 7.0\n",
    "\n",
    "    return torch.from_numpy(grid.ravel()).float()  # shape (H*W,)\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=256, n_actions=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def run_episode(env, model, device=None):\n",
    "    \"\"\"\n",
    "    Run one episode using the current stochastic policy.\n",
    "    Returns: trajectory list[(state_tensor_cpu, action_int)], total_reward\n",
    "    - Uses state_to_flat_grid_with_facing(state, H, W) as encoder.\n",
    "    - Auto-detects device from model if device is None.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    traj = []\n",
    "    state = env.get_state()\n",
    "    done = state[\"done\"]\n",
    "    total = 0\n",
    "\n",
    "    while not done:\n",
    "        # encode state as flat H*W tensor (keeps wumpus facing)\n",
    "        inp = state_to_tensor(state, env.h, env.w).to(device)  # (H*W,)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp.unsqueeze(0))  # (1,4)\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze(0)  # (4,)\n",
    "            action = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        reward, done = env.step(action)\n",
    "        total += reward\n",
    "\n",
    "        traj.append((inp.cpu(), action))  # store CPU tensor for batching later\n",
    "        state = env.get_state()\n",
    "\n",
    "    return traj, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6be1cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3660863637924194\n",
      "loss: 1.2563667297363281\n",
      "loss: 1.1404964923858643\n",
      "loss: 1.0120081901550293\n",
      "loss: 0.9068971276283264\n",
      "loss: 0.8261556029319763\n",
      "loss: 0.7783100605010986\n",
      "loss: 0.7341163158416748\n",
      "loss: 0.7168760895729065\n",
      "loss: 0.6945194602012634\n",
      "Epoch 000  meanR -98.5  bestR 97.0  elite_steps 249\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m     19\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 20\u001b[0m     traj, total \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     episodes\u001b[38;5;241m.\u001b[39mappend(traj)\n\u001b[0;32m     22\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(total)\n",
      "Cell \u001b[1;32mIn[10], line 79\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(env, model, device)\u001b[0m\n\u001b[0;32m     75\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# encode state as flat H*W tensor (keeps wumpus facing)\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[43mstate_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (H*W,)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     82\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(inp\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# (1,4)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PolicyNet(8 * 8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "train_epochs = 100\n",
    "batch_size = 64\n",
    "elite_frac = 0.3\n",
    "\n",
    "\n",
    "env = WumpusEnv(w=8, h=8, n_pits=6, n_wumpus=3, seed=42, wumpus_orientation=\"down\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1) Collect batch_size episodes using current policy\n",
    "    episodes = []\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        env.reset()\n",
    "        traj, total = run_episode(env, model)\n",
    "        episodes.append(traj)\n",
    "        rewards.append(total)\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "\n",
    "    k = int(batch_size * elite_frac)\n",
    "    elite_idx = rewards.argsort()[-k:]\n",
    "    elite_trajs = [episodes[i] for i in elite_idx]\n",
    "\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    for traj in elite_trajs:\n",
    "        for s, a in traj:\n",
    "            states_list.append(s.numpy())\n",
    "            actions_list.append(a)\n",
    "    if len(states_list) == 0:\n",
    "        print(f\"Epoch {epoch}: no elite data, skipping\")\n",
    "        continue\n",
    "    states = torch.tensor(np.stack(states_list), dtype=torch.float32)  # (N, input_dim)\n",
    "    actions = torch.tensor(actions_list, dtype=torch.long)  # (N,)\n",
    "\n",
    "    for i in range(train_epochs):\n",
    "        opt.zero_grad()\n",
    "        a_predict = model(states)  # (N, 4)\n",
    "        loss = criterion(a_predict, actions)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if  i % 10 == 0:\n",
    "            print(f\"loss: {loss}\")\n",
    "\n",
    "    mean_r = rewards.mean()\n",
    "    best_r = rewards.max()\n",
    "    print(f\"Epoch {epoch:03d}  meanR {mean_r:.1f}  bestR {best_r:.1f}  elite_steps {len(states_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
